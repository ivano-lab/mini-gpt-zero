# mini-gpt-zero

Implementação de um modelo de linguagem simples inspirado na arquitetura dos Transformers (como o GPT). Voltado para o aprendizado de como modelos de IA geradores de texto funcionam na prática.

## Objetivos

- Compreender os fundamentos de modelos de linguagem.
- Implementar um pequeno transformer em PyTorch.
- Treinar o modelo para prever o próximo caractere de um texto.
- Explorar como a geração de texto funciona por meio de redes neurais.

## Tecnologias usadas

- Python
- PyTorch
- NumPy

## Instalação

```bash
pip install torch numpy tqdm
```
## Em breve
Treinamento com texto real

- Geração de texto

- Explicações teóricas em paralelo ao código

- Visualizações do aprendizado