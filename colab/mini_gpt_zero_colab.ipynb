{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mini GPT Zero — Construção Didática de um Modelo de Linguagem\n",
        "\n",
        "Este notebook tem como objetivo entender profundamente, passo a passo, como funciona um modelo de linguagem do tipo **GPT** (*Generative Pretrained Transformer*), sem depender de bibliotecas complexas.\n",
        "\n",
        "Foco didático: **cada parte do código será explicada em detalhes**, para solidificar tanto o entendimento teórico quanto a prática de implementação.  \n",
        "*Este notebook está em construção contínua como parte de um estudo autodirigido para reduzir débito técnico em IA, LLMs e fundamentos de deep learning.*\n",
        "\n",
        "---\n",
        "\n",
        "## Organização do notebook\n",
        "\n",
        "1. **Título e introdução do projeto**\n",
        "2. **Seção de bibliotecas**\n",
        "3. **Upload ou leitura do corpus**\n",
        "4. **Tokenização e preparação dos dados**\n",
        "5. **Função de batching**\n",
        "6. **Construção do modelo passo a passo (camadas do Transformer)**\n",
        "7. **Treinamento básico**\n",
        "8. **Geração de texto**\n",
        "9. **Conclusão e próximos passos**\n",
        "\n",
        "---\n",
        "\n",
        "## Com explicações comentadas\n",
        "\n",
        "- Cada célula de código será seguida de um texto explicando o que foi feito, por que foi feito e como funciona.\n",
        "\n",
        "---\n",
        "\n",
        "## Objetivo final\n",
        "\n",
        "- Criar um **notebook funcional e didático**, que servirá como base para um artigo técnico documentando todo o processo.\n"
      ],
      "metadata": {
        "id": "ZhKY2CWmo5Et"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_tqeVcZjp5I"
      },
      "outputs": [],
      "source": [
        "# Mini GPT Zero — Construindo um GPT didático no Google Colab\n",
        "## Documentação interativa e código comentado do projeto."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nova seção"
      ],
      "metadata": {
        "id": "uiGdAQIwsG59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Config:\n",
        "    vocab_size = 113         # tamanho do vocabulário (ajuste com base no seu)\n",
        "    block_size = 8           # tamanho da sequência de entrada\n",
        "    n_embed = 64             # dimensão do embedding\n",
        "    n_heads = 4              # número de cabeças de atenção\n",
        "    n_layers = 2             # número de blocos transformer\n",
        "    dropout = 0.1            # taxa de dropout\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# --------------------------\n",
        "# Embedding + Atenção\n",
        "# --------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(config.n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(config.n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embed, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)     # (B, T, C)\n",
        "        q = self.query(x)   # (B, T, C)\n",
        "        wei = q @ k.transpose(-2, -1) / (C ** 0.5)  # (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "# --------------------------\n",
        "# Multi-Head Attention\n",
        "# --------------------------\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(config.n_embed, config.n_embed)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "# --------------------------\n",
        "# Feedforward\n",
        "# --------------------------\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --------------------------\n",
        "# Bloco Transformer\n",
        "# --------------------------\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_heads):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_heads\n",
        "        self.sa = MultiHeadAttention(n_heads, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# --------------------------\n",
        "# Modelo GPT simplificado\n",
        "# --------------------------\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(config.n_embed, config.n_heads) for _ in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embed)\n",
        "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)             # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, C)\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            return logits\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "            return logits, loss"
      ],
      "metadata": {
        "id": "hZD-tdP_s2r1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monta o drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjQL9bNf8qfh",
        "outputId": "c6cbb4b2-adcb-4dba-fb07-ca9ebbde6f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Declaração da classe MiniGPT\n",
        "model = MiniGPT().to(device)\n",
        "\n",
        "# Caminho do corpus no drive\n",
        "# DATA_PATH = Path(\"/content/drive/caminho-para-seus-arquivos-no-drive-nem-fundendo-que-exponho-o-meu-aqui\")\n",
        "\n",
        "# Hiperparâmetros iniciais\n",
        "block_size = 8   # tamanho do contexto (número de caracteres anteriores)\n",
        "batch_size = 4   # número de sequências por batch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Baixar o arquivo corpus.txt do GitHub usando wget\n",
        "!wget -O corpus.txt \"https://raw.githubusercontent.com/ivano-lab/mini-gpt-zero/refs/heads/main/data/corpus.txt\"\n",
        "\n",
        "# 1. Leitura do texto\n",
        "with open('corpus.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# 2. Tokenização caractere a caractere\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocabulário: {vocab_size} caracteres\")\n",
        "\n",
        "# Mapas de conversão\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "# Funções auxiliares\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Texto codificado\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(\"Trecho original:\")\n",
        "print(text[:2000])\n",
        "print(\"\\nCodificado:\")\n",
        "print(encode(text[:2000]))\n",
        "\n",
        "# 3. Criação dos batches\n",
        "def get_batch(split):\n",
        "    # separa entre treino e validação (90/10)\n",
        "    split_idx = int(0.9 * len(data))\n",
        "    data_split = data[:split_idx] if split == 'train' else data[split_idx:]\n",
        "\n",
        "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
        "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "# Exemplo de uso\n",
        "x_batch, y_batch = get_batch('train')\n",
        "print(\"Entrada (x):\")\n",
        "print(x_batch)\n",
        "print(\"Como texto:\")\n",
        "for i in range(batch_size):\n",
        "    print(decode(x_batch[i].tolist()), \"->\", decode(y_batch[i].tolist()))\n",
        "\n",
        "model = MiniGPT().to(device)  # importante enviar o modelo para o dispositivo correto (CPU ou GPU)\n",
        "\n",
        "# Rodar o modelo com um forward pass usando o mesmo batch já obtido\n",
        "logits, loss = model(x_batch, y_batch)\n",
        "\n",
        "print(\"Forma do input (x):\", x_batch.shape)\n",
        "print(\"Forma do target (y):\", y_batch.shape)\n",
        "print(\"Forma dos logits:\", logits.shape)\n",
        "print(\"Loss inicial:\", loss.item())"
      ],
      "metadata": {
        "id": "tij28EKssHgQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf65df24-4579-46a3-80ec-43a663ee154d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-26 01:29:59--  https://raw.githubusercontent.com/ivano-lab/mini-gpt-zero/refs/heads/main/data/corpus.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 418217 (408K) [text/plain]\n",
            "Saving to: ‘corpus.txt’\n",
            "\n",
            "corpus.txt          100%[===================>] 408.42K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-06-26 01:29:59 (9.01 MB/s) - ‘corpus.txt’ saved [418217/418217]\n",
            "\n",
            "Vocabulário: 113 caracteres\n",
            "Trecho original:\n",
            "﻿The Project Gutenberg eBook of Dom Casmurro\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n",
            "\n",
            "Title: Dom Casmurro\n",
            "\n",
            "\n",
            "Author: Machado de Assis\n",
            "\n",
            "Release date: October 15, 2017 [eBook #55752]\n",
            "\n",
            "Language: Portuguese\n",
            "\n",
            "\n",
            "\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK DOM CASMURRO ***\n",
            "\n",
            "\n",
            "\n",
            "Produced by Laura Natal Rodriguez & Marc D'Hooghe at Free\n",
            "Literature (online soon in an extended version,also linking\n",
            "to free sources for education worldwide ... MOOC's,\n",
            "educational materials,...) (Images generously made available\n",
            "by the Bibliotheca Nacional Digital Brasil.)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DOM CASMURRO\n",
            "\n",
            "POR\n",
            "\n",
            "MACHADO DE ASSIS\n",
            "\n",
            "DA ACADEMIA BRAZILEIRA\n",
            "\n",
            "H. GARNIER, LIVREIRO-EDITOR\n",
            "\n",
            "RUA MOREIRA CEZAR, 71\n",
            "\n",
            "RIO DE JANEIRO\n",
            "\n",
            "6, RUE DES SAINTS-PÈRES, 6\n",
            "\n",
            "PARIZ\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "I\n",
            "\n",
            "Do titulo.\n",
            "\n",
            "Uma noite destas, vindo da cidade para o Engenho Novo, encontrei no\n",
            "trem da Central um rapaz aqui do bairro, que eu conheço de vista e\n",
            "de chapéo. Comprimentou-me, sentou-se ao pé de mim, falou da lua e\n",
            "dos ministros, e acabou recitando-me versos. A viagem era curta, e os\n",
            "versos póde ser que não fossem inteiramente maus. Succedeu, porém, que\n",
            "como eu estava cançado, fechei os olhos tres ou quatro vezes; tanto\n",
            "bastou para que elle interrompesse a leitura e mettesse os versos no\n",
            "bolso.\n",
            "\n",
            "--Continue, disse eu accordando.\n",
            "\n",
            "--Já acabei, murmurou elle.\n",
            "\n",
            "--São muito bonitos.\n",
            "\n",
            "Vi-lhe fazer um gesto para tiral-os outra vez do bolso, mas não passou\n",
            "do gesto; estava amuado. No dia seguinte entrou a dizer de mim nomes\n",
            "feios, e acabou alcunhando-me _Dom Casmurro._ Os visinhos, que não\n",
            "gostam dos meus habitos reclusos e calados, deram curso á alcunha, que\n",
            "afinal pegou\n",
            "\n",
            "Codificado:\n",
            "[112, 49, 66, 63, 1, 45, 76, 73, 68, 63, 61, 78, 1, 36, 79, 78, 63, 72, 60, 63, 76, 65, 1, 63, 31, 73, 73, 69, 1, 73, 64, 1, 33, 73, 71, 1, 32, 59, 77, 71, 79, 76, 76, 73, 0, 1, 1, 1, 1, 0, 49, 66, 67, 77, 1, 63, 60, 73, 73, 69, 1, 67, 77, 1, 64, 73, 76, 1, 78, 66, 63, 1, 79, 77, 63, 1, 73, 64, 1, 59, 72, 83, 73, 72, 63, 1, 59, 72, 83, 81, 66, 63, 76, 63, 1, 67, 72, 1, 78, 66, 63, 1, 50, 72, 67, 78, 63, 62, 1, 48, 78, 59, 78, 63, 77, 1, 59, 72, 62, 0, 71, 73, 77, 78, 1, 73, 78, 66, 63, 76, 1, 74, 59, 76, 78, 77, 1, 73, 64, 1, 78, 66, 63, 1, 81, 73, 76, 70, 62, 1, 59, 78, 1, 72, 73, 1, 61, 73, 77, 78, 1, 59, 72, 62, 1, 81, 67, 78, 66, 1, 59, 70, 71, 73, 77, 78, 1, 72, 73, 1, 76, 63, 77, 78, 76, 67, 61, 78, 67, 73, 72, 77, 0, 81, 66, 59, 78, 77, 73, 63, 80, 63, 76, 14, 1, 54, 73, 79, 1, 71, 59, 83, 1, 61, 73, 74, 83, 1, 67, 78, 12, 1, 65, 67, 80, 63, 1, 67, 78, 1, 59, 81, 59, 83, 1, 73, 76, 1, 76, 63, 13, 79, 77, 63, 1, 67, 78, 1, 79, 72, 62, 63, 76, 1, 78, 66, 63, 1, 78, 63, 76, 71, 77, 0, 73, 64, 1, 78, 66, 63, 1, 45, 76, 73, 68, 63, 61, 78, 1, 36, 79, 78, 63, 72, 60, 63, 76, 65, 1, 41, 67, 61, 63, 72, 77, 63, 1, 67, 72, 61, 70, 79, 62, 63, 62, 1, 81, 67, 78, 66, 1, 78, 66, 67, 77, 1, 63, 60, 73, 73, 69, 1, 73, 76, 1, 73, 72, 70, 67, 72, 63, 0, 59, 78, 1, 81, 81, 81, 14, 65, 79, 78, 63, 72, 60, 63, 76, 65, 14, 73, 76, 65, 14, 1, 38, 64, 1, 83, 73, 79, 1, 59, 76, 63, 1, 72, 73, 78, 1, 70, 73, 61, 59, 78, 63, 62, 1, 67, 72, 1, 78, 66, 63, 1, 50, 72, 67, 78, 63, 62, 1, 48, 78, 59, 78, 63, 77, 12, 0, 83, 73, 79, 1, 81, 67, 70, 70, 1, 66, 59, 80, 63, 1, 78, 73, 1, 61, 66, 63, 61, 69, 1, 78, 66, 63, 1, 70, 59, 81, 77, 1, 73, 64, 1, 78, 66, 63, 1, 61, 73, 79, 72, 78, 76, 83, 1, 81, 66, 63, 76, 63, 1, 83, 73, 79, 1, 59, 76, 63, 1, 70, 73, 61, 59, 78, 63, 62, 0, 60, 63, 64, 73, 76, 63, 1, 79, 77, 67, 72, 65, 1, 78, 66, 67, 77, 1, 63, 31, 73, 73, 69, 14, 0, 0, 49, 67, 78, 70, 63, 26, 1, 33, 73, 71, 1, 32, 59, 77, 71, 79, 76, 76, 73, 0, 0, 0, 30, 79, 78, 66, 73, 76, 26, 1, 42, 59, 61, 66, 59, 62, 73, 1, 62, 63, 1, 30, 77, 77, 67, 77, 0, 0, 47, 63, 70, 63, 59, 77, 63, 1, 62, 59, 78, 63, 26, 1, 44, 61, 78, 73, 60, 63, 76, 1, 17, 21, 12, 1, 18, 16, 17, 23, 1, 56, 63, 31, 73, 73, 69, 1, 3, 21, 21, 23, 21, 18, 57, 0, 0, 41, 59, 72, 65, 79, 59, 65, 63, 26, 1, 45, 73, 76, 78, 79, 65, 79, 63, 77, 63, 0, 0, 0, 0, 10, 10, 10, 1, 48, 49, 30, 47, 49, 1, 44, 35, 1, 49, 37, 34, 1, 45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 1, 34, 31, 44, 44, 40, 1, 33, 44, 42, 1, 32, 30, 48, 42, 50, 47, 47, 44, 1, 10, 10, 10, 0, 0, 0, 0, 45, 76, 73, 62, 79, 61, 63, 62, 1, 60, 83, 1, 41, 59, 79, 76, 59, 1, 43, 59, 78, 59, 70, 1, 47, 73, 62, 76, 67, 65, 79, 63, 84, 1, 6, 1, 42, 59, 76, 61, 1, 33, 7, 37, 73, 73, 65, 66, 63, 1, 59, 78, 1, 35, 76, 63, 63, 0, 41, 67, 78, 63, 76, 59, 78, 79, 76, 63, 1, 8, 73, 72, 70, 67, 72, 63, 1, 77, 73, 73, 72, 1, 67, 72, 1, 59, 72, 1, 63, 82, 78, 63, 72, 62, 63, 62, 1, 80, 63, 76, 77, 67, 73, 72, 12, 59, 70, 77, 73, 1, 70, 67, 72, 69, 67, 72, 65, 0, 78, 73, 1, 64, 76, 63, 63, 1, 77, 73, 79, 76, 61, 63, 77, 1, 64, 73, 76, 1, 63, 62, 79, 61, 59, 78, 67, 73, 72, 1, 81, 73, 76, 70, 62, 81, 67, 62, 63, 1, 14, 14, 14, 1, 42, 44, 44, 32, 7, 77, 12, 0, 63, 62, 79, 61, 59, 78, 67, 73, 72, 59, 70, 1, 71, 59, 78, 63, 76, 67, 59, 70, 77, 12, 14, 14, 14, 9, 1, 8, 38, 71, 59, 65, 63, 77, 1, 65, 63, 72, 63, 76, 73, 79, 77, 70, 83, 1, 71, 59, 62, 63, 1, 59, 80, 59, 67, 70, 59, 60, 70, 63, 0, 60, 83, 1, 78, 66, 63, 1, 31, 67, 60, 70, 67, 73, 78, 66, 63, 61, 59, 1, 43, 59, 61, 67, 73, 72, 59, 70, 1, 33, 67, 65, 67, 78, 59, 70, 1, 31, 76, 59, 77, 67, 70, 14, 9, 0, 0, 0, 0, 0, 0, 33, 44, 42, 1, 32, 30, 48, 42, 50, 47, 47, 44, 0, 0, 45, 44, 47, 0, 0, 42, 30, 32, 37, 30, 33, 44, 1, 33, 34, 1, 30, 48, 48, 38, 48, 0, 0, 33, 30, 1, 30, 32, 30, 33, 34, 42, 38, 30, 1, 31, 47, 30, 55, 38, 41, 34, 38, 47, 30, 0, 0, 37, 14, 1, 36, 30, 47, 43, 38, 34, 47, 12, 1, 41, 38, 51, 47, 34, 38, 47, 44, 13, 34, 33, 38, 49, 44, 47, 0, 0, 47, 50, 30, 1, 42, 44, 47, 34, 38, 47, 30, 1, 32, 34, 55, 30, 47, 12, 1, 23, 17, 0, 0, 47, 38, 44, 1, 33, 34, 1, 39, 30, 43, 34, 38, 47, 44, 0, 0, 22, 12, 1, 47, 50, 34, 1, 33, 34, 48, 1, 48, 30, 38, 43, 49, 48, 13, 45, 89, 47, 34, 48, 12, 1, 22, 0, 0, 45, 30, 47, 38, 55, 0, 0, 0, 0, 0, 38, 0, 0, 33, 73, 1, 78, 67, 78, 79, 70, 73, 14, 0, 0, 50, 71, 59, 1, 72, 73, 67, 78, 63, 1, 62, 63, 77, 78, 59, 77, 12, 1, 80, 67, 72, 62, 73, 1, 62, 59, 1, 61, 67, 62, 59, 62, 63, 1, 74, 59, 76, 59, 1, 73, 1, 34, 72, 65, 63, 72, 66, 73, 1, 43, 73, 80, 73, 12, 1, 63, 72, 61, 73, 72, 78, 76, 63, 67, 1, 72, 73, 0, 78, 76, 63, 71, 1, 62, 59, 1, 32, 63, 72, 78, 76, 59, 70, 1, 79, 71, 1, 76, 59, 74, 59, 84, 1, 59, 75, 79, 67, 1, 62, 73, 1, 60, 59, 67, 76, 76, 73, 12, 1, 75, 79, 63, 1, 63, 79, 1, 61, 73, 72, 66, 63, 95, 73, 1, 62, 63, 1, 80, 67, 77, 78, 59, 1, 63, 0, 62, 63, 1, 61, 66, 59, 74, 97, 73, 14, 1, 32, 73, 71, 74, 76, 67, 71, 63, 72, 78, 73, 79, 13, 71, 63, 12, 1, 77, 63, 72, 78, 73, 79, 13, 77, 63, 1, 59, 73, 1, 74, 97, 1, 62, 63, 1, 71, 67, 71, 12, 1, 64, 59, 70, 73, 79, 1, 62, 59, 1, 70, 79, 59, 1, 63, 0, 62, 73, 77, 1, 71, 67, 72, 67, 77, 78, 76, 73, 77, 12, 1, 63, 1, 59, 61, 59, 60, 73, 79, 1, 76, 63, 61, 67, 78, 59, 72, 62, 73, 13, 71, 63, 1, 80, 63, 76, 77, 73, 77, 14, 1, 30, 1, 80, 67, 59, 65, 63, 71, 1, 63, 76, 59, 1, 61, 79, 76, 78, 59, 12, 1, 63, 1, 73, 77, 0, 80, 63, 76, 77, 73, 77, 1, 74, 101, 62, 63, 1, 77, 63, 76, 1, 75, 79, 63, 1, 72, 94, 73, 1, 64, 73, 77, 77, 63, 71, 1, 67, 72, 78, 63, 67, 76, 59, 71, 63, 72, 78, 63, 1, 71, 59, 79, 77, 14, 1, 48, 79, 61, 61, 63, 62, 63, 79, 12, 1, 74, 73, 76, 97, 71, 12, 1, 75, 79, 63, 0, 61, 73, 71, 73, 1, 63, 79, 1, 63, 77, 78, 59, 80, 59, 1, 61, 59, 72, 95, 59, 62, 73, 12, 1, 64, 63, 61, 66, 63, 67, 1, 73, 77, 1, 73, 70, 66, 73, 77, 1, 78, 76, 63, 77, 1, 73, 79, 1, 75, 79, 59, 78, 76, 73, 1, 80, 63, 84, 63, 77, 27, 1, 78, 59, 72, 78, 73, 0, 60, 59, 77, 78, 73, 79, 1, 74, 59, 76, 59, 1, 75, 79, 63, 1, 63, 70, 70, 63, 1, 67, 72, 78, 63, 76, 76, 73, 71, 74, 63, 77, 77, 63, 1, 59, 1, 70, 63, 67, 78, 79, 76, 59, 1, 63, 1, 71, 63, 78, 78, 63, 77, 77, 63, 1, 73, 77, 1, 80, 63, 76, 77, 73, 77, 1, 72, 73, 0, 60, 73, 70, 77, 73, 14, 0, 0, 13, 13, 32, 73, 72, 78, 67, 72, 79, 63, 12, 1, 62, 67, 77, 77, 63, 1, 63, 79, 1, 59, 61, 61, 73, 76, 62, 59, 72, 62, 73, 14, 0, 0, 13, 13, 39, 93, 1, 59, 61, 59, 60, 63, 67, 12, 1, 71, 79, 76, 71, 79, 76, 73, 79, 1, 63, 70, 70, 63, 14, 0, 0, 13, 13, 48, 94, 73, 1, 71, 79, 67, 78, 73, 1, 60, 73, 72, 67, 78, 73, 77, 14, 0, 0, 51, 67, 13, 70, 66, 63, 1, 64, 59, 84, 63, 76, 1, 79, 71, 1, 65, 63, 77, 78, 73, 1, 74, 59, 76, 59, 1, 78, 67, 76, 59, 70, 13, 73, 77, 1, 73, 79, 78, 76, 59, 1, 80, 63, 84, 1, 62, 73, 1, 60, 73, 70, 77, 73, 12, 1, 71, 59, 77, 1, 72, 94, 73, 1, 74, 59, 77, 77, 73, 79, 0, 62, 73, 1, 65, 63, 77, 78, 73, 27, 1, 63, 77, 78, 59, 80, 59, 1, 59, 71, 79, 59, 62, 73, 14, 1, 43, 73, 1, 62, 67, 59, 1, 77, 63, 65, 79, 67, 72, 78, 63, 1, 63, 72, 78, 76, 73, 79, 1, 59, 1, 62, 67, 84, 63, 76, 1, 62, 63, 1, 71, 67, 71, 1, 72, 73, 71, 63, 77, 0, 64, 63, 67, 73, 77, 12, 1, 63, 1, 59, 61, 59, 60, 73, 79, 1, 59, 70, 61, 79, 72, 66, 59, 72, 62, 73, 13, 71, 63, 1, 58, 33, 73, 71, 1, 32, 59, 77, 71, 79, 76, 76, 73, 14, 58, 1, 44, 77, 1, 80, 67, 77, 67, 72, 66, 73, 77, 12, 1, 75, 79, 63, 1, 72, 94, 73, 0, 65, 73, 77, 78, 59, 71, 1, 62, 73, 77, 1, 71, 63, 79, 77, 1, 66, 59, 60, 67, 78, 73, 77, 1, 76, 63, 61, 70, 79, 77, 73, 77, 1, 63, 1, 61, 59, 70, 59, 62, 73, 77, 12, 1, 62, 63, 76, 59, 71, 1, 61, 79, 76, 77, 73, 1, 93, 1, 59, 70, 61, 79, 72, 66, 59, 12, 1, 75, 79, 63, 0, 59, 64, 67, 72, 59, 70, 1, 74, 63, 65, 73, 79]\n",
            "Entrada (x):\n",
            "tensor([[61, 73, 72, 78, 67, 72, 79, 73],\n",
            "        [73, 79,  1, 75, 79, 59, 78, 76],\n",
            "        [67, 71, 59,  1, 36, 70, 73, 76],\n",
            "        [73,  0, 64, 67, 84, 63, 76, 59]])\n",
            "Como texto:\n",
            "continuo -> ontinuou\n",
            "ou quatr -> u quatro\n",
            "ima Glor -> ma Glori\n",
            "o\n",
            "fizera -> \n",
            "fizera \n",
            "Forma do input (x): torch.Size([4, 8])\n",
            "Forma do target (y): torch.Size([4, 8])\n",
            "Forma dos logits: torch.Size([32, 113])\n",
            "Loss inicial: 4.934825897216797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nova seção"
      ],
      "metadata": {
        "id": "Olq_JAN4syfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição do modelo Transformer (MiniGPT)\n",
        "Aqui implementamos as classes e funções que compõem o modelo de linguagem. Este modelo é inspirado no GPT e é responsável por:\n",
        "- Criar os embeddings dos tokens\n",
        "- Aplicar atenção\n",
        "- Processar as saídas do Transformer\n",
        "- Gerar logits e loss para o treinamento\n"
      ],
      "metadata": {
        "id": "EC_LUAjTszUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nova seção"
      ],
      "metadata": {
        "id": "N7peKuU-s13c"
      }
    }
  ]
}